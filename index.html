<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ExpressMind: A Multimodal Pretrained Large Language Model for Expressway Operation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57pOZl+62Q+FPKCa20HHFD+pC+pCr1+aI/gWFEudZo+cWI1" crossorigin="anonymous">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
  <body>
    <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ExpressMind: A Multimodal Pretrained Large Language Model for Expressway Operation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Zihe Wang</a><sup>a</sup>,</span>
                <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yihuan Wang</a><sup>a</sup>,</span>
                  <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Haiyang Yu</a><sup>*,a</sup>,</span>
                  <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Zhiyong Cui</a><sup>a</sup>,</span>
                <span class="author-block">
                <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Xiaojian Liao</a><sup>a</sup>,</span>
                  <span class="author-block">
                <a href="FIFTH AUTHOR PERSONAL LINK" target="_blank">Chengcheng Wang</a><sup>b</sup>,</span>
                    <span class="author-block">
                <a href="SIXTH AUTHOR PERSONAL LINK" target="_blank">Yonglin Tian</a><sup>c</sup>,</span>
                      <span class="author-block">
                <a href="SEVENTH AUTHOR PERSONAL LINK" target="_blank">Yongxin Tong</a><sup>a</sup>,</span>
                        <span class="author-block">
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>a</sup>Beihang University, 100191, Beijing, P.R.China.</span><br>
                    
                    <span class="author-block"><sup>b</sup>Shandong Hi-speed Group Co., Ltd, 250098, Jinan, P.R.China.</span><br>
                    
                    <span class="author-block"><sup>c</sup>Institute of automation, Chinese Academy of Sciences, Beijing, P.R.China.</span>
                    
                    <span class="eql-cntrb"><small><br><sup></sup>First author: by2313310@buaa.edu.cn</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding author: zhiyongc@buaa.edu.cn, hyyu@buaa.edu.cn</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/ExpressMind.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/wanderhee/ExpressMind-Code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code coming soon</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- on-ramp video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here -->
        <!-- <source src="static/videos/test_exp_1_4_2-episode-0.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The scenario of merging onto the highway from an entrance ramp.
      </h2>
    </div>
  </div>
</section> -->
<!-- End on-ramp video -->

<!-- Paper abstract -->
<section class="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The current expressway operation relies on rule-based and isolated models, which limits the ability to jointly analyze knowledge across different systems. Meanwhile, Large Language Models (LLMs) are increasingly applied in intelligent transportation, advancing traffic models from algorithmic to cognitive intelligence. However, general LLMs are unable to effectively understand the regulations and causal relationships of events in unconventional scenarios in the expressway field. Therefore, this paper constructs a pre-trained multimodal large language model (MLLM) for expressways, ExpressMind, which serves as the cognitive core for intelligent expressway operations. This paper constructs the industry’s first full-stack expressway dataset, encompassing traffic knowledge texts, emergency reasoning chains, and annotated video events to overcome data scarcity. This paper proposes a dual-layer LLM pre-training paradigm based on self-supervised training and unsupervised learning. Additionally, this study introduces a Graph-Augmented RAG framework to dynamically index the expressway knowledge base. To enhance reasoning for expressway incident response strategies, we develop a RL-aligned Chain-of-Thought (RL-CoT) mechanism that enforces consistency between model reasoning and expert problem-solving heuristics for incident handling. Finally, ExpressMind integrates a cross-modal encoder to align the dynamic feature sequences under the visual and textual channels, enabling it to understand traffic scenes in both video and image modalities. Extensive experiments on our newly released multi-modal expressway benchmark demonstrate that ExpressMind comprehensively outperforms existing baselines in event detection, safety response generation, and complex traffic analysis. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Main Contributions</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<!-- Main Contributions -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
          <div class="level-set has-text-justified">
            <p class="contribution-paragraph">
              Our contributions can be mainly divided into the following parts:<br>

              1.Full-stack Expressway dataset: This study constructs the first industry's full-stack expressway dataset spanning text cognition, logical reasoning, and visual perception, including three specialized subsets: traffic knowledge texts, emergency response reasoning, and event video scene understanding.<br>
                
              2.RL-aligned CoT Reasoning: We design a RL-based expressway strategy alignment strategy in LLM training, which can significantly enhance the model's logical reasoning and self-correction capabilities.<br>
                
              3.Graph-Augmented Retrieval: A graph RAG-based dynamic knowledge base is established for critical expressway information retrieval and indexing.<br>
                
              4.Multimodal Alignment mechanism: A Visual-Prior Alignment mechanism is designed by enforcing alignment and reweighting of visual tokens to enhance the understanding of visual features.<br>
                
              5.Multi-modal Benchmark: The multi-modal Benchmark for evaluating LLMs within the expressway domain is released, encompassing four evaluation subsets: basic knowledge comprehension, video incident detection, safety response generation, and traffic analysis reporting.<br>
              </p>
          </div>
      </div>
    </div>
  </div>


<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Framework Overview of the ExpressMind</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<!-- Method Overview -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <!-- <h2 class="title is-3">Method Overview</h2> -->
          <center>
          <img src="static/images/1.png" alt="Framework" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              this paper introduces ExpressMind, a domain multimodal LLM for expressway operation. We construct the first full-stack expressway dataset and propose a two-stage pre-training paradigm for the internalization of expressway-domain knowledge. This study also develops a Reinforcement Learning (RL)-based Chain-of-Thought (CoT) alignment mechanism to strengthen domain reasoning. Furthermore, a visual-enhanced cross-modal encoder is incorporated and a graph-based retrieval-augmented generation (RAG) is proposed to enhance the extraction of key traffic scene characteristics and dynamic knowledge.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Method Overview -->
<section class="Overview">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <!-- <h2 class="title is-3">Method Overview</h2> -->
          <center>
          <img src="static/images/2.png" alt="Framework" class="center-image blend-img-background"/>
          </center>
          <div class="level-set has-text-justified">
            <p>
              This study introduces ExpressMind, the first domain-specific MLLM designed for expressway scenarios. It is built through multiple technical innovations: a two-stage pretraining paradigm for domain knowledge internalization, a GRPO-enhanced RL framework for safety-critical reasoning alignment, a graph-augmented RAG mechanism for real-time spatiotemporal knowledge retrieval, and a VPA multimodal alignment module for deep video understanding. To support this work, we have open-sourced the first training dataset covering domain knowledge, incident CoT strategy reasoning, and multimodal incident detection VQA. The ExpressMind has been applied in top-tier expressway groups, serving as a representative application case of large models in the expressway domain.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero is-light is-small">
  <div class="hero-body">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <h3 class="title is-3" style="text-align: center; padding-bottom: 5px; color: rgb(63	172	38)"><bold>Video</bold></h3>
      </div>
    </div>
  </div>
  </div>
</section>

<style>
  /* 新增样式 */
  .code-container11 {
    max-width: 960px;        /* 控制最大宽度 */
    margin: 2rem auto;       /* 上下留白 + 水平居中 */
    background: #f8f9fa00;
  }
  
  .code-container11 pre {
    margin: 0;               /* 清除默认外边距 */
    font-size: 0.9em;        /* 调小字体 */
    line-height: 1.4;        /* 紧凑行高 */
    white-space: pre-wrap;   /* 保留换行同时允许自动换行 */
    word-break: break-all;   /* 强制换行策略 */
  }
  
  /* Bulma 框架适配 */
  .container.is-max-desktop .content11 {
    display: flex;
    justify-content: center; /* 水平居中 */
  }
  </style>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="level-set has-text-justified">
      <p>
        The ExpressMind-VL intelligent operation system has been deployed in practical applications for multi-task scenarios on expressways. The visualization system, as shown in the figure/video, includes functions such as releasing warning information, summarizing traffic conditions, describing video events, and generating handling recommendations. We have deployed the system on the expressways in Shandong and Zhejiang provinces. In the intelligent management of Shandong expressways, ExpressMind-VL classifies the types and severity levels of traffic surveillance videos, and generates analytical reports along with handling strategies for traffic incidents. For the intelligent management of Guangdong expressways, ExpressMind-VL detects traffic events based on real-time video streams and produces structured textual descriptions for comprehension. 
      </p>
    </div>

    <!-- 修改1：给 hero-body 添加 flex 布局 -->
    <div class="hero-body is-flex is-flex-direction-column is-align-items-center">
      <!-- 修改2：限制视频最大宽度 -->
      <video 
        poster="" 
        id="tree" 
        autoplay 
        controls 
        muted 
        loop 
        height="100%"
        style="max-width: 80%; width: auto;" <!-- 控制视频宽度 -->
      >
        <source src="static/videos/1.mp4" type="video/mp4">
      </video>
    </div>
  </div>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Declaration of competing interest: The authors declare that they have no known competing financial interests or personal relationships that could have
appeared to influence the work reported in this paper
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
    
</body>
</html>
